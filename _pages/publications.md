---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

I’m currently a PhD student in the Department of Statistics at the University of Oxford, studying as part of the [StatML CDT](https://statml.io/). I’m supervised by [Arnaud Doucet](https://www.stats.ox.ac.uk/~doucet/) and [George Deligiannidis](https://www.stats.ox.ac.uk/~deligian/), and my work focuses on developing a theoretical understanding of generative modelling techniques, with a particular focus on diffusion models and variational autoencoders.

During my first year of the CDT, I worked on a couple of research projects. The first aimed to generalise denoising diffusion models to arbitrary state spaces, with particular applications to discrete spaces. The second focused on infinite-width limits of neural networks, and their use in studying the inductive biases and training dynamics of neural networks. Since then, I’ve also worked on variational and importance weighted autoencoders, investigating the properties of variational bound in high dimensions.

## Publications

[A Continuous Time Framework for Discrete Denoising Models](https://arxiv.org/abs/2205.14987). Andrew Campbell, **Joe Benton**, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, Arnaud Doucet. _arXiv preprint, arXiv:2205.14987_

<!-- ## Talks

Some talks will go here -->
