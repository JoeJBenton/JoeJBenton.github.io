---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

I'm currently a member of technical staff on the Alignment Science team at [Anthropic](https://www.anthropic.com/), where I work on adversarial testing of large language models and think about how we might evaluate the alignment properties of very powerful ML systems. Previously, I was a PhD student in the [Department of Statistics](https://www.stats.ox.ac.uk/) at the University of Oxford, where I was supervised by [Arnaud Doucet](https://www.stats.ox.ac.uk/~doucet/) and [George Deligiannidis](https://www.stats.ox.ac.uk/~deligian/) and worked on the theory of diffusion models. I've also spent time with the [UK Frontier AI Taskforce](https://www.gov.uk/government/publications/frontier-ai-taskforce-second-progress-report) (now the UK AI Safety Institute) and remain excited about building government capacity to understand and regulate frontier AI systems.

## Publications

[Many-shot Jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking). Cem Anil, Esin Durmus, Mrinank Sharma, **Joe Benton**, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi et al. _Anthropic Blog_, 2024.

[From Denoising Diffusions to Denoising Markov Models](https://academic.oup.com/jrsssb/advance-article/doi/10.1093/jrsssb/qkae005/7564909). **Joe Benton**, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 86(2):286−301, 2024.

[Nearly d-Linear Convergence Bounds for Diffusion Models via Stochastic Localization](https://arxiv.org/abs/2308.03686). **Joe Benton**, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis. _International Conference on Learning Representations_, 2024.

[Error Bounds for Flow Matching Methods](https://arxiv.org/abs/2305.16860). **Joe Benton**, George Deligiannidis, Arnaud Doucet. _Transactions on Machine Learning Research_, February 2024.

[Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics](https://www.jmlr.org/papers/volume24/22-1160/22-1160.pdf). Kamélia Daudel, **Joe Benton**\*, Yuyang Shi\*, Arnaud Doucet. _Journal of Machine Learning Research_, 24(243):1−83, 2023.

[Measuring Feature Sparsity in Language Models](https://arxiv.org/abs/2310.07837). Mingyang Deng, Lucas Tao, **Joe Benton**. _NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research_, 2023.

[A Continuous Time Framework for Discrete Denoising Models](https://papers.nips.cc/paper_files/paper/2022/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html). Andrew Campbell, **Joe Benton**, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, Arnaud Doucet. _Advances in Neural Information Processing Systems_, 2022.

[Polysemanticity and Capacity in Neural Networks](https://arxiv.org/abs/2210.01892). Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, **Joe Benton**, Buck Shlegeris. _arXiv preprint, arXiv:2210.01892_, 2022.

<!-- ## Talks

Some talks will go here -->
