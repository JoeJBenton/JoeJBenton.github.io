---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

I'm currently a member of technical staff on the Alignment Science team at [Anthropic](https://www.anthropic.com/), where I think about how we might make safety cases for very powerful ML systems and evaluate their alignment relevant properties. Previously, I was a PhD student in the [Department of Statistics](https://www.stats.ox.ac.uk/) at the University of Oxford, where I was supervised by [Arnaud Doucet](https://www.stats.ox.ac.uk/~doucet/) and [George Deligiannidis](https://www.stats.ox.ac.uk/~deligian/) and worked on the theory of diffusion models. I've also spent time with the [UK Frontier AI Taskforce](https://www.gov.uk/government/publications/frontier-ai-taskforce-second-progress-report) (now the UK AI Safety Institute) and remain excited about building government capacity to understand and regulate frontier AI systems.

## Projects

I'm currently looking for potential collaborators on two projects:

- [Building a High-Quality Control Dataset](https://docs.google.com/document/d/1okUYcfjSxkzptbMYb-5ABqz-Lg-lwLrKC0QhlQN9_6s/edit?tab=t.0#heading=h.kp1dywkdwlm6)

- [Eliciting and Measuring Steganography](https://docs.google.com/document/d/1GqrBIc6DIzarfADWSbsvDvG_GUag40OV88sx3BSljxM/edit?tab=t.0#heading=h.t4phonbuw8f1)

If you're highly motivated to help make transformative AI go well, have a background in empirical ML research, AI control or dataset curation, and want to work on these projects, reach out to me at joe [at] anthropic [dot] com with a description of your interests, or consider applying to the [Anthropic Safety Fellowship](https://alignment.anthropic.com/2024/anthropic-fellows-program/).

## Publications

[SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740). Jonathan Kutasov, Yuqi Sun, Paul Colognese, Teun van der Weij, Linda Petrini, Chen Bo Calvin Zhang, John Hughes, Xiang Deng, Henry Sleight, Tyler Tracy, Buck Shlegeris, **Joe Benton**. *arXiv preprint arXiv:2506.15740*, 2025.

[Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410). Yanda Chen, **Joe Benton**, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez. *arXiv preprint arXiv:2505.05410*, 2025.

[Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417). Aryo Pradipta Gema, Alexander Hägele, Runjin Chen, Andy Arditi, Jacob Goldman-Wetzler, Kit Fraser-Taliente, Henry Sleight, Linda Petrini, Julian Michael, Beatrice Alex, Pasquale Minervini, Yanda Chen, **Joe Benton**, Ethan Perez. *arXiv preprint arXiv:2507.14417*, 2025.

[Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473). Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, **Joe Benton**, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Dan Hendrycks, et al. *arXiv preprint arXiv:2507.11473*, 2025.

[Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777). Miles Turpin, Andy Arditi, Meihua Li, **Joe Benton**, Julian Michael. *arXiv preprint arXiv:2506.22777*, 2025.

[Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming](https://www.anthropic.com/research/constitutional-classifiers). Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, **Joe Benton**, Emma Bluemke, et al. _arXiv preprint arXiv:2501.18837_, 2025.

[Sabotage Evaluations for Frontier Models](https://arxiv.org/abs/2410.21514). **Joe Benton**, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris et al. _arXiv preprint arXiv:2410.21514_, 2024.

[When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?](https://arxiv.org/abs/2407.15211). Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durante, **Joe Benton**, Brando Miranda, Henry Sleight, John Hughes et al. _NeurIPS 2024 Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models_, 2024.

[Many-shot Jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking). Cem Anil, Esin Durmus, Mrinank Sharma, **Joe Benton**, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford et al. _Advances in Neural Information Processing Systems_, 2024.

[From Denoising Diffusions to Denoising Markov Models](https://academic.oup.com/jrsssb/advance-article/doi/10.1093/jrsssb/qkae005/7564909). **Joe Benton**, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 86(2):286−301, 2024.

[Nearly d-Linear Convergence Bounds for Diffusion Models via Stochastic Localization](https://arxiv.org/abs/2308.03686). **Joe Benton**, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis. _International Conference on Learning Representations_, 2024.

[Error Bounds for Flow Matching Methods](https://arxiv.org/abs/2305.16860). **Joe Benton**, George Deligiannidis, Arnaud Doucet. _Transactions on Machine Learning Research_, February 2024.

[Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics](https://www.jmlr.org/papers/volume24/22-1160/22-1160.pdf). Kamélia Daudel, **Joe Benton**\*, Yuyang Shi\*, Arnaud Doucet. _Journal of Machine Learning Research_, 24(243):1−83, 2023.

[Measuring Feature Sparsity in Language Models](https://arxiv.org/abs/2310.07837). Mingyang Deng, Lucas Tao, **Joe Benton**. _NeurIPS 2023 Workshop on Socially Responsible Language Modelling Research_, 2023.

[A Continuous Time Framework for Discrete Denoising Models](https://papers.nips.cc/paper_files/paper/2022/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html). Andrew Campbell, **Joe Benton**, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, Arnaud Doucet. _Advances in Neural Information Processing Systems_, 2022.

[Polysemanticity and Capacity in Neural Networks](https://arxiv.org/abs/2210.01892). Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, **Joe Benton**, Buck Shlegeris. _arXiv preprint, arXiv:2210.01892_, 2022.

<!-- ## Talks

Some talks will go here -->
